{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972763c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl pandas nltk scikit-learn spacy matplotlib PyPDF2 PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188f72a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed84158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openpyxl\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import spacy\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from svglib.svglib import svg2rlg\n",
    "from reportlab.graphics import renderPDF\n",
    "import matplotlib.pyplot as plt\n",
    "import fitz\n",
    "\n",
    "# Function to convert Excel to CSV\n",
    "def excel_to_csv(input_file, output_file):\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"Error: {input_file} does not exist.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        workbook = openpyxl.load_workbook(input_file, read_only=True)\n",
    "        sheet = workbook.active\n",
    "\n",
    "        with open(output_file, \"w\", newline=\"\") as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            for row in sheet.iter_rows(values_only=True):\n",
    "                writer.writerow(row)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {input_file}: {e}\")\n",
    "    finally:\n",
    "        workbook.close()\n",
    "\n",
    "# Convert Excel to CSV\n",
    "excel_to_csv(\"AI_Engineer_Dataset_Task_1.xlsx\", \"AI_Engineer_Dataset_Task_1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83df9a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the second Excel file\n",
    "df_courses = pd.read_excel(\"AI_Engineer_Dataset_Task_2.xlsx\")\n",
    "\n",
    "# Define chunk size for reading large datasets\n",
    "chunk_size = 100000\n",
    "\n",
    "# Mapping for ParticipantResponse\n",
    "response_mapping = {\n",
    "    'Strongly Disagree': 1,\n",
    "    'Disagree': 2,\n",
    "    'Neutral': 3,\n",
    "    'Agree': 4,\n",
    "    'Strongly Agree': 5,\n",
    "    'no': 3\n",
    "}\n",
    "\n",
    "# Define column data types for memory efficiency\n",
    "col_types = {\n",
    "    'CourseCode': str,\n",
    "    'CourseName': str,\n",
    "    'ParticipantResponse': str\n",
    "}\n",
    "\n",
    "# Initialize an empty DataFrame for concatenating the chunks\n",
    "df_responses = pd.DataFrame()\n",
    "\n",
    "\n",
    "# Read and process the CSV in chunks\n",
    "for chunk in pd.read_csv(\"AI_Engineer_Dataset_Task_1.csv\", dtype=col_types, usecols=list(col_types.keys()), chunksize=chunk_size):\n",
    "    chunk['Score'] = chunk['ParticipantResponse'].map(response_mapping)\n",
    "    chunk_merged = chunk.merge(df_courses, on=['CourseCode', 'CourseName'], how='left')\n",
    "    df_responses = pd.concat([df_responses, chunk_merged], ignore_index=True)\n",
    "\n",
    "# Now, df_responses_processed contains your fully processed data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b26805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... previous parts\n",
    "\n",
    "# Now, df_responses_processed contains your fully processed data\n",
    "\n",
    "# Part 3: Handle non-string and missing values\n",
    "\n",
    "# Check for any non-string values in the 'ParticipantResponse' column\n",
    "non_string_rows = df_responses[df_responses['ParticipantResponse'].apply(lambda x: not isinstance(x, str))]\n",
    "\n",
    "# Option 1: Fill NaN values with a default string\n",
    "df_responses['ParticipantResponse'].fillna('No Response', inplace=True)\n",
    "\n",
    "# Option 2 (alternative to Option 1, if you prefer to drop rows): \n",
    "# df_responses_processed.dropna(subset=['ParticipantResponse'], inplace=True)\n",
    "\n",
    "# Now, df_responses_processed is cleaned up and ready for further analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db95514b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install openpyxl\n",
    "print(\"df_responses is initialized: \", 'df_responses' in locals())\n",
    "\n",
    "\n",
    "# Part 4: Data Preprocessing, Sentiment Analysis, and Topic Modeling\n",
    "\n",
    "# Initialize spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def preprocess_texts(texts):\n",
    "    return [\" \".join([token.lemma_ for token in doc if not token.is_stop and token.is_alpha]) for doc in nlp.pipe(texts, batch_size=500)]\n",
    "\n",
    "# Process 'ParticipantResponse' column\n",
    "df_responses['ProcessedResponse'] = preprocess_texts(df_responses['ParticipantResponse'])\n",
    "\n",
    "# Sentiment Analysis using NLTK's VADER sentiment intensity analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "df_responses['Sentiment'] = df_responses['ProcessedResponse'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "# Feature Extraction using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf = vectorizer.fit_transform(df_responses['ProcessedResponse'])\n",
    "\n",
    "# Topic Modeling using NMF (Non-Negative Matrix Factorization)\n",
    "n_topics = 5  # You can change this number based on your specific needs\n",
    "nmf = NMF(n_components=n_topics, random_state=42).fit(tfidf)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display topics\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(f\"Topic #{topic_idx + 1}:\")\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[-10:]]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418caeb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "from reportlab.graphics import renderPDF\n",
    "from svglib.svglib import svg2rlg\n",
    "import matplotlib.pyplot as plt\n",
    "import fitz \n",
    "\n",
    "df_responses = pd.read_csv(\"processed_responses.csv\")\n",
    "\n",
    "# Initialize variables\n",
    "pdf_file = \"sentiment_analysis_report.pdf\"\n",
    "c = canvas.Canvas(pdf_file, pagesize=letter)\n",
    "c.setFont(\"Helvetica\", 12)  # Set initial font size to 12\n",
    "\n",
    "# Add Title\n",
    "c.drawString(100, 800, \"Sentiment Analysis Report\")\n",
    "\n",
    "# Print Results\n",
    "best_courses = df_responses.groupby('CourseName')['Sentiment'].mean().sort_values(ascending=False).head(5)\n",
    "worst_courses = df_responses.groupby('CourseName')['Sentiment'].mean().sort_values().head(5)\n",
    "college_feedback = df_responses.groupby('College')['Sentiment'].mean().sort_values(ascending=False)\n",
    "degree_feedback = df_responses.groupby('DegreeName')['Sentiment'].mean().sort_values(ascending=False)\n",
    "\n",
    "\n",
    "# Add results to the PDF\n",
    "y_position = 750\n",
    "for section_title, data in [(\"Courses with Best Feedback\", best_courses),\n",
    "                            (\"Courses with Worst Feedback\", worst_courses),\n",
    "                            (\"Feedback by College\", college_feedback),\n",
    "                            (\"Feedback by Degree\", degree_feedback)]:\n",
    "    c.setFont(\"Helvetica\", 10)\n",
    "    c.drawString(100, y_position, section_title + \":\")\n",
    "    y_position -= 20\n",
    "    for idx, (label, value) in enumerate(data.items(), start=1):\n",
    "        c.drawString(120, y_position, f\"{idx}. {label}: {value:.2f}\")\n",
    "        y_position -= 15\n",
    "        \n",
    "        # Visualize the feedback text using spaCy's displacy.render\n",
    "        doc = nlp(label)  # Assuming 'label' contains the feedback text\n",
    "        \n",
    "        # Create the directory if it doesn't exist\n",
    "        if not os.path.exists(\"feedback_images\"):\n",
    "            os.makedirs(\"feedback_images\")\n",
    "\n",
    "        # Save the visualization as an SVG image file\n",
    "        image_file = f\"feedback_images/feedback_{idx - 1}.svg\"\n",
    "        svg = displacy.render(doc, style=\"dep\", jupyter=False, options={'compact': True})\n",
    "        with open(image_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(svg)\n",
    "\n",
    "        # Embed the visualization SVG in the PDF using ReportLab's drawSvg function\n",
    "        drawing = svg2rlg(image_file)\n",
    "        drawing.scale(0.2, 0.2) \n",
    "        renderPDF.draw(drawing, c, 400, y_position + 15)\n",
    "\n",
    "        y_position -= 10\n",
    "\n",
    "c.showPage()\n",
    "\n",
    "# Add section for NLP Techniques\n",
    "y_position = 750  # Starting Y-position\n",
    "c.setFont(\"Helvetica\", 8)\n",
    "c.drawString(100, y_position, \"NLP Techniques Used:\")\n",
    "y_position -= 20\n",
    "nlp_techniques = [\n",
    "    \"Data Preprocessing: Lemmatization, removal of stop words, and filtering to alphabetic tokens.\",\n",
    "    \"Sentiment Analysis: Used VADER sentiment analyzer to evaluate the polarity of the processed text.\",\n",
    "    \"Feature Extraction: Employed TF-IDF to transform the text data into feature vectors.\",\n",
    "    \"Topic Modeling: Applied NMF for extracting the underlying topics.\"\n",
    "]\n",
    "\n",
    "for technique in nlp_techniques:\n",
    "    c.setFont(\"Helvetica\", 8)\n",
    "    c.drawString(120, y_position, technique)\n",
    "    y_position -= 20\n",
    "\n",
    "# Add a space between sections\n",
    "y_position -= 20\n",
    "\n",
    "# Add section for Findings\n",
    "c.setFont(\"Helvetica\", 10)\n",
    "c.drawString(100, y_position, \"Findings:\")\n",
    "y_position -= 20\n",
    "findings = [\n",
    "    \"Best and Worst Courses: 'Course A' received the highest sentiment score, suggesting positive feedback.\",\n",
    "    \"Feedback by College: 'College X' had the most positive feedback, whereas 'College Y' had the most negative.\",\n",
    "    \"Feedback by Degree: Feedback for 'Master's Degree' courses was generally more positive than that for 'Bachelor's Degree' courses.\"\n",
    "]\n",
    "\n",
    "for finding in findings:\n",
    "    c.drawString(120, y_position, finding)\n",
    "    y_position -= 20\n",
    "    \n",
    "\n",
    "    \n",
    "# Save the PDF\n",
    "c.save()\n",
    "\n",
    "# Visualize the distribution of sentiment scores and save it as a separate PDF\n",
    "fig, ax = plt.subplots()\n",
    "plt.hist(df_responses['Sentiment'], bins=30, alpha=0.75)\n",
    "plt.title('Distribution of Sentiment Scores')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Number of Responses')\n",
    "plot_pdf_file = \"sentiment_distribution.pdf\"\n",
    "plt.savefig(plot_pdf_file, format='pdf')\n",
    "plt.close(fig)\n",
    "\n",
    "# Combine the main PDF report and the plot PDF using PyMuPDF\n",
    "main_pdf = fitz.open(pdf_file)\n",
    "plot_pdf = fitz.open(plot_pdf_file)\n",
    "output_pdf = fitz.open()\n",
    "\n",
    "output_pdf.insert_pdf(main_pdf)\n",
    "output_pdf.insert_pdf(plot_pdf)\n",
    "\n",
    "output_pdf.save(\"combined_report.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f0599f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
