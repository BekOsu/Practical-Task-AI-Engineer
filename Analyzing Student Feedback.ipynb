{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ed84158",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: svglib in ./anaconda3/lib/python3.11/site-packages (1.5.1)\n",
      "Requirement already satisfied: reportlab in ./anaconda3/lib/python3.11/site-packages (from svglib) (4.0.4)\n",
      "Requirement already satisfied: lxml in ./anaconda3/lib/python3.11/site-packages (from svglib) (4.9.1)\n",
      "Requirement already satisfied: tinycss2>=0.6.0 in ./anaconda3/lib/python3.11/site-packages (from svglib) (1.2.1)\n",
      "Requirement already satisfied: cssselect2>=0.2.0 in ./anaconda3/lib/python3.11/site-packages (from svglib) (0.7.0)\n",
      "Requirement already satisfied: webencodings in ./anaconda3/lib/python3.11/site-packages (from cssselect2>=0.2.0->svglib) (0.5.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in ./anaconda3/lib/python3.11/site-packages (from reportlab->svglib) (9.4.0)\n",
      "Requirement already satisfied: reportlab in ./anaconda3/lib/python3.11/site-packages (4.0.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in ./anaconda3/lib/python3.11/site-packages (from reportlab) (9.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/abu/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import openpyxl\n",
    "import csv\n",
    "import os\n",
    "import pandas as pd \n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "from reportlab.lib.pagesizes import letter\n",
    "from reportlab.pdfgen import canvas\n",
    "from io import BytesIO\n",
    "import fitz \n",
    "from spacy import displacy\n",
    "from reportlab.lib.utils import ImageReader\n",
    "from svglib.svglib import svg2rlg\n",
    "from reportlab.graphics import renderPDF\n",
    "!pip install svglib\n",
    "!pip install reportlab\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "\n",
    "def excel_to_csv(input_file, output_file):\n",
    "    \"\"\"Convert an Excel file to a CSV file without loading the entire file into memory.\"\"\"\n",
    "    if not os.path.exists(input_file):\n",
    "        print(f\"Error: {input_file} does not exist.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        workbook = openpyxl.load_workbook(input_file, read_only=True)\n",
    "        sheet = workbook.active\n",
    "\n",
    "        with open(output_file, \"w\", newline=\"\") as csv_file:\n",
    "            writer = csv.writer(csv_file)\n",
    "            for row in sheet.iter_rows(values_only=True):\n",
    "                writer.writerow(row)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {input_file}: {e}\")\n",
    "    finally:\n",
    "        workbook.close()\n",
    "\n",
    "excel_to_csv(\"AI_Engineer_Dataset_Task_1.xlsx\", \"AI_Engineer_Dataset_Task_1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83df9a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the smaller dataset directly once and use it in subsequent parts.\n",
    "df_courses = pd.read_excel(\"AI_Engineer_Dataset_Task_2.xlsx\")\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 100000  # Adjust this value based on your system's capabilities\n",
    "\n",
    "# Convert ParticipantResponse to numerical scores\n",
    "response_mapping = {\n",
    "    'Strongly Disagree': 1,\n",
    "    'Disagree': 2,\n",
    "    'Neutral': 3,\n",
    "    'Agree': 4,\n",
    "    'Strongly Agree': 5,\n",
    "    'no': 3  # Assuming \"no\" is a neutral response.\n",
    "}\n",
    "\n",
    "# Define dtypes for efficient memory usage\n",
    "col_types = {\n",
    "    'CourseCode': str,\n",
    "    'CourseName': str,\n",
    "    'ParticipantResponse': str  # Assuming this is the column you're mapping\n",
    "}\n",
    "\n",
    "# Initialize an empty dataframe to store the concatenated result\n",
    "df_responses = pd.DataFrame()\n",
    "\n",
    "# Read the CSV in chunks and process each chunk\n",
    "for chunk in pd.read_csv(\"AI_Engineer_Dataset_Task_1.csv\", dtype=col_types, usecols=list(col_types.keys()), chunksize=chunk_size):\n",
    "    chunk['Score'] = chunk['ParticipantResponse'].map(response_mapping)\n",
    "    chunk_merged = chunk.merge(df_courses, on=['CourseCode', 'CourseName'], how='left')\n",
    "    df_responses = pd.concat([df_responses, chunk_merged], ignore_index=True)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94b26805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for any non-string values in the column\n",
    "non_string_rows = df_responses[df_responses['ParticipantResponse'].apply(lambda x: not isinstance(x, str))]\n",
    "# print(non_string_rows)\n",
    "\n",
    "# Option 1: Fill NaN values with a default string\n",
    "df_responses['ParticipantResponse'].fillna('No Response', inplace=True)\n",
    "\n",
    "# Option 2 (alternative to Option 1, if we prefer to drop rows): \n",
    "# df_responses.dropna(subset=['ParticipantResponse'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "db95514b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1:\n",
      "الدكتور method الشرح شرحه وايد مفهوم number statement agree strongly\n",
      "\n",
      "\n",
      "Topic #2:\n",
      "مفهوم وايد write right far evaluate teaching method survey disagree\n",
      "\n",
      "\n",
      "Topic #3:\n",
      "شافي كل لدي المساق تعليق توجد شكرا يوجد لا neutral\n",
      "\n",
      "\n",
      "Topic #4:\n",
      "instructor semester number statement write right far evaluate survey agree\n",
      "\n",
      "\n",
      "Topic #5:\n",
      "شافي كل لدي المساق تعليق توجد شكرا يوجد لا response\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize spaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "def preprocess_texts(texts):\n",
    "    return [\" \".join([token.lemma_ for token in doc if not token.is_stop and token.is_alpha]) for doc in nlp.pipe(texts, batch_size=500)]\n",
    "\n",
    "# Process 'ParticipantResponse' column\n",
    "df_responses['ProcessedResponse'] = preprocess_texts(df_responses['ParticipantResponse'])\n",
    "\n",
    "# Sentiment Analysis using NLTK's VADER sentiment intensity analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "df_responses['Sentiment'] = df_responses['ProcessedResponse'].apply(lambda x: sia.polarity_scores(x)['compound'])\n",
    "\n",
    "# Save the processed DataFrame to a CSV file\n",
    "df_responses.to_csv(\"processed_responses.csv\", index=False)\n",
    "    \n",
    "# Feature Extraction using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, stop_words='english')\n",
    "tfidf = vectorizer.fit_transform(df_responses['ProcessedResponse'])\n",
    "\n",
    "# Topic Modeling using NMF (Non-Negative Matrix Factorization)\n",
    "n_topics = 5\n",
    "nmf = NMF(n_components=n_topics, random_state=42).fit(tfidf)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display topics\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    print(f\"Topic #{topic_idx + 1}:\")\n",
    "    print(\" \".join([feature_names[i] for i in topic.argsort()[-10:]]))\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "418caeb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load the processed DataFrame from part 4\n",
    "df_responses = pd.read_csv(\"processed_responses.csv\")\n",
    "\n",
    "# Print Results\n",
    "best_courses = df_responses.groupby('CourseName')['Sentiment'].mean().sort_values(ascending=False).head(5)\n",
    "worst_courses = df_responses.groupby('CourseName')['Sentiment'].mean().sort_values().head(5)\n",
    "college_feedback = df_responses.groupby('College')['Sentiment'].mean().sort_values(ascending=False)\n",
    "degree_feedback = df_responses.groupby('DegreeName')['Sentiment'].mean().sort_values(ascending=False)\n",
    "\n",
    "# Create a PDF report\n",
    "pdf_file = \"sentiment_analysis_report.pdf\"\n",
    "c = canvas.Canvas(pdf_file, pagesize=letter)\n",
    "c.setFont(\"Helvetica\", 12)  # Set initial font size to 12\n",
    "c.drawString(100, 800, \"Sentiment Analysis Report\")\n",
    "\n",
    "\n",
    "# Add results to the PDF\n",
    "y_position = 750\n",
    "for section_title, data in [(\"Courses with Best Feedback\", best_courses),\n",
    "                            (\"Courses with Worst Feedback\", worst_courses),\n",
    "                            (\"Feedback by College\", college_feedback),\n",
    "                            (\"Feedback by Degree\", degree_feedback)]:\n",
    "    c.setFont(\"Helvetica\", 10)\n",
    "    c.drawString(100, y_position, section_title + \":\")\n",
    "    y_position -= 20\n",
    "    for idx, (label, value) in enumerate(data.items(), start=1):\n",
    "        c.drawString(120, y_position, f\"{idx}. {label}: {value:.2f}\")\n",
    "        y_position -= 15\n",
    "        \n",
    "        # Visualize the feedback text using spaCy's displacy.render\n",
    "        doc = nlp(label)  # Assuming 'label' contains the feedback text\n",
    "        \n",
    "        # Create the directory if it doesn't exist\n",
    "        if not os.path.exists(\"feedback_images\"):\n",
    "            os.makedirs(\"feedback_images\")\n",
    "\n",
    "        # Save the visualization as an SVG image file\n",
    "        image_file = f\"feedback_images/feedback_{idx - 1}.svg\"\n",
    "        svg = displacy.render(doc, style=\"dep\", jupyter=False, options={'compact': True})\n",
    "        with open(image_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(svg)\n",
    "\n",
    "        # Embed the visualization SVG in the PDF using ReportLab's drawSvg function\n",
    "        drawing = svg2rlg(image_file)\n",
    "        drawing.scale(0.2, 0.2) \n",
    "        renderPDF.draw(drawing, c, 400, y_position + 15)\n",
    "\n",
    "        y_position -= 10\n",
    "\n",
    "# Save the PDF\n",
    "c.save()\n",
    "\n",
    "# Visualize the distribution of sentiment scores and save it as a separate PDF\n",
    "fig, ax = plt.subplots()\n",
    "plt.hist(df_responses['Sentiment'], bins=30, alpha=0.75)\n",
    "plt.title('Distribution of Sentiment Scores')\n",
    "plt.xlabel('Sentiment Score')\n",
    "plt.ylabel('Number of Responses')\n",
    "plot_pdf_file = \"sentiment_distribution.pdf\"\n",
    "plt.savefig(plot_pdf_file, format='pdf')\n",
    "plt.close(fig)\n",
    "\n",
    "# Combine the main PDF report and the plot PDF using PyMuPDF\n",
    "main_pdf = fitz.open(pdf_file)\n",
    "plot_pdf = fitz.open(plot_pdf_file)\n",
    "output_pdf = fitz.open()\n",
    "\n",
    "output_pdf.insert_pdf(main_pdf)\n",
    "output_pdf.insert_pdf(plot_pdf)\n",
    "\n",
    "output_pdf.save(\"combined_report.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f0599f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
